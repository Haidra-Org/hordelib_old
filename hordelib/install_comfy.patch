diff --git a/comfy/k_diffusion/sampling.py b/comfy/k_diffusion/sampling.py
index c809d39..9c9f6b8 100644
--- a/comfy/k_diffusion/sampling.py
+++ b/comfy/k_diffusion/sampling.py
@@ -8,6 +8,8 @@ import torchsde
 from tqdm.auto import trange, tqdm
 
 from . import utils
+from loguru import logger
+import threading
 
 
 def append_zero(x):
@@ -584,24 +586,36 @@ def sample_dpmpp_sde(model, x, sigmas, extra_args=None, callback=None, disable=N
 @torch.no_grad()
 def sample_dpmpp_2m(model, x, sigmas, extra_args=None, callback=None, disable=None):
     """DPM-Solver++(2M)."""
+    # logger.warning(f"Starting DPM-Solver++(2M) on model {id(model):x}")
     extra_args = {} if extra_args is None else extra_args
     s_in = x.new_ones([x.shape[0]])
     sigma_fn = lambda t: t.neg().exp()
     t_fn = lambda sigma: sigma.log().neg()
     old_denoised = None
 
+    # logger.warning(f"Starting iterations of DPM-Solver++(2M) on model {id(model):x}")
     for i in trange(len(sigmas) - 1, disable=disable):
+        # logger.warning(f"Starting iterations {i} on model {id(model):x}")
+        # logger.warning(f"denoise = {id(model):x}({id(x):x}, {id(sigmas):x} * {id(s_in)}, {id(extra_args)})")
         denoised = model(x, sigmas[i] * s_in, **extra_args)
+        # logger.warning(f"  denoised on model {id(model):x}")
         if callback is not None:
             callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})
+        # logger.warning(f"  step 1 {id(model):x}")
         t, t_next = t_fn(sigmas[i]), t_fn(sigmas[i + 1])
+        # logger.warning(f"  step 2 {id(model):x}")
         h = t_next - t
+        # logger.warning(f"  step 3 {id(model):x}")
         if old_denoised is None or sigmas[i + 1] == 0:
+            # logger.warning(f"  step 3a {id(model):x}")
             x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised
         else:
+            # logger.warning(f"  step 3b {id(model):x}")
             h_last = t - t_fn(sigmas[i - 1])
             r = h_last / h
             denoised_d = (1 + 1 / (2 * r)) * denoised - (1 / (2 * r)) * old_denoised
             x = (sigma_fn(t_next) / sigma_fn(t)) * x - (-h).expm1() * denoised_d
+        # logger.warning(f"  step 4 end iteration {id(model):x}")
         old_denoised = denoised
+    # logger.warning(f"Done DPM-Solver++(2M) on model {id(model):x}")
     return x
diff --git a/comfy/model_management.py b/comfy/model_management.py
index a0d1313..f919f06 100644
--- a/comfy/model_management.py
+++ b/comfy/model_management.py
@@ -1,6 +1,9 @@
 import psutil
 from enum import Enum
 from cli_args import args
+import threading
+from loguru import logger
+from contextlib import nullcontext
 
 class VRAMState(Enum):
     CPU = 0
@@ -30,13 +33,6 @@ try:
     except:
         total_vram = torch.cuda.mem_get_info(torch.cuda.current_device())[1] / (1024 * 1024)
     total_ram = psutil.virtual_memory().total / (1024 * 1024)
-    if not args.normalvram and not args.cpu:
-        if total_vram <= 4096:
-            print("Trying to enable lowvram mode because your GPU seems to have 4GB or less. If you don't want this use: --normalvram")
-            set_vram_to = VRAMState.LOW_VRAM
-        elif total_vram > total_ram * 1.1 and total_vram > 14336:
-            print("Enabling highvram mode because your GPU has more vram than your computer has ram. If you don't want this use: --normalvram")
-            vram_state = VRAMState.HIGH_VRAM
 except:
     pass
 
@@ -59,10 +55,10 @@ else:
             print("xformers version:", XFORMERS_VERSION)
             if XFORMERS_VERSION.startswith("0.0.18"):
                 print()
-                print("WARNING: This version of xformers has a major bug where you will get black images when generating high resolution images.")
-                print("Please downgrade or upgrade xformers to a different version.")
-                print()
-                XFORMERS_ENABLED_VAE = False
+                # print("WARNING: This version of xformers has a major bug where you will get black images when generating high resolution images.")
+                # print("Please downgrade or upgrade xformers to a different version.")
+                # print()
+                # XFORMERS_ENABLED_VAE = False
         except:
             pass
     except:
@@ -110,94 +106,180 @@ except:
 if args.cpu:
     vram_state = VRAMState.CPU
 
-print(f"Set vram state to: {vram_state.name}")
+# print(f"Set vram state to: {vram_state.name}")
 
 
-current_loaded_model = None
-current_gpu_controlnets = []
+class ModelManager:
+    _instance = None
+    _initialised = False
+    _load_mutex = threading.RLock()
+    _property_mutex = threading.RLock()
+    sampler_mutex = threading.RLock()
+    vae_mutex = threading.RLock()
 
-model_accelerated = False
+    _property_mutex = nullcontext()
 
+    user_reserved_vram_mb = 0
 
-def unload_model():
-    global current_loaded_model
-    global model_accelerated
-    global current_gpu_controlnets
-    global vram_state
+    # We are a singleton
+    def __new__(cls):
+        if cls._instance is None:
+            cls._instance = super().__new__(cls)
+        return cls._instance
 
-    if current_loaded_model is not None:
-        if model_accelerated:
-            accelerate.hooks.remove_hook_from_submodules(current_loaded_model.model)
-            model_accelerated = False
+    # We initialise only ever once (in the lifetime of the singleton)
+    def __init__(self):
+        if not self._initialised:
+            self.models_in_use = []
+            self.current_loaded_models = []
+            self.current_gpu_controlnets = []
+            self.models_accelerated = []
+            self.__class__._initialised = True    
 
-        #never unload models from GPU on high vram
-        if vram_state != VRAMState.HIGH_VRAM:
-            current_loaded_model.model.cpu()
-        current_loaded_model.unpatch_model()
-        current_loaded_model = None
+    def set_user_reserved_vram(self, vram_mb):
+        with self._property_mutex:
+            self.user_reserved_vram_mb = vram_mb
 
-    if vram_state != VRAMState.HIGH_VRAM:
-        if len(current_gpu_controlnets) > 0:
-            for n in current_gpu_controlnets:
-                n.cpu()
-            current_gpu_controlnets = []
+    def get_models_on_gpu(self):
+        with self._property_mutex:
+            return self.current_loaded_models[:]
 
+    def set_model_in_use(self, model):
+        with self._property_mutex:
+            self.models_in_use.append(model)
 
-def load_model_gpu(model):
-    global current_loaded_model
-    global vram_state
-    global model_accelerated
+    def is_model_in_use(self, model):
+        with self._property_mutex:
+            return model in self.models_in_use
 
-    if model is current_loaded_model:
-        return
-    unload_model()
-    try:
-        real_model = model.patch_model()
-    except Exception as e:
-        model.unpatch_model()
-        raise e
-    current_loaded_model = model
-    if vram_state == VRAMState.CPU:
-        pass
-    elif vram_state == VRAMState.MPS:
-        mps_device = torch.device("mps")
-        real_model.to(mps_device)
-        pass
-    elif vram_state == VRAMState.NORMAL_VRAM or vram_state == VRAMState.HIGH_VRAM:
-        model_accelerated = False
-        real_model.to(get_torch_device())
-    else:
-        if vram_state == VRAMState.NO_VRAM:
-            device_map = accelerate.infer_auto_device_map(real_model, max_memory={0: "256MiB", "cpu": "16GiB"})
-        elif vram_state == VRAMState.LOW_VRAM:
-            device_map = accelerate.infer_auto_device_map(real_model, max_memory={0: "{}MiB".format(total_vram_available_mb), "cpu": "16GiB"})
+    def unload_model(self, model):
+        global vram_state
+        with self._property_mutex:
+            if model not in self.current_loaded_models:
+                logger.debug("Skip GPU unload as not on the GPU")
+                return
 
-        accelerate.dispatch_model(real_model, device_map=device_map, main_device=get_torch_device())
-        model_accelerated = True
-    return current_loaded_model
+            if model in self.models_in_use:
+                logger.debug("Not unloaded model as it is in use right now")
+                return
 
-def load_controlnet_gpu(control_models):
-    global current_gpu_controlnets
-    global vram_state
-    if vram_state == VRAMState.CPU:
-        return
+            if model in self.models_accelerated:
+                accelerate.hooks.remove_hook_from_submodules(model.model)
+                self.models_accelerated.remove(model)
 
-    if vram_state == VRAMState.LOW_VRAM or vram_state == VRAMState.NO_VRAM:
-        #don't load controlnets like this if low vram because they will be loaded right before running and unloaded right after
-        return
+            self.current_loaded_models.remove(model)
 
-    models = []
-    for m in control_models:
-        models += m.get_models()
-
-    for m in current_gpu_controlnets:
-        if m not in models:
-            m.cpu()
+        # Unload to RAM
+        model.model.cpu()
+        model.unpatch_model()
+        return True
 
-    device = get_torch_device()
-    current_gpu_controlnets = []
-    for m in models:
-        current_gpu_controlnets.append(m.to(device))
+    def done_with_model(self, model):
+        with self._property_mutex:
+            if model in self.models_in_use:
+                self.models_in_use.remove(model)
+
+    def have_free_vram(self):
+        freemem = round(get_free_memory(get_torch_device()) / (1024 * 1024))
+        logger.debug(f"Free VRAM is: {freemem}MB ({len(self.current_loaded_models)} models loaded on GPU)")
+        return freemem > self.user_reserved_vram_mb
+
+    def load_model_gpu(self, model):
+        global vram_state
+        
+        with self._load_mutex:
+            #logger.warning(f"load_model_gpu( {id(model):x} )")
+
+            # Don't run out of vram
+            if self.current_loaded_models:
+                if not self.have_free_vram():
+                    # Release the first least used model that we can
+                    for release_model in reversed(self.current_loaded_models):
+                        if self.unload_model(release_model):
+                            break
+                    freemem = round(get_free_memory(get_torch_device()) / (1024 * 1024))
+                    logger.debug(f"Unloaded a model, free VRAM is now: {freemem}MB ({len(self.current_loaded_models)} models loaded on GPU)")
+
+            if model in self.current_loaded_models:
+                # Move this model to the top of the list
+                self.current_loaded_models.insert(0, self.current_loaded_models.pop(self.current_loaded_models.index(model)))
+                #logger.warning(f"Model {id(model):x} already on GPU so not loading")
+                return model
+            
+            try:
+                #logger.warning(f"Patching model {id(model):x}")
+                real_model = model.patch_model()
+            except Exception as e:
+                logger.error("Patching failed")
+                model.unpatch_model()
+                raise e
+            
+            #logger.warning(f"Adding model to current_loaded_models {id(model):x}")
+            self.current_loaded_models.insert(0, model)
+
+            if vram_state == VRAMState.CPU:
+                pass
+            elif vram_state == VRAMState.MPS:
+                mps_device = torch.device("mps")
+                real_model.to(mps_device)
+            elif vram_state == VRAMState.NORMAL_VRAM or vram_state == VRAMState.HIGH_VRAM:
+                if model in self.models_accelerated:
+                    #logger.warning(f"removing model from accelerated list {id(model):x}")
+                    self.models_accelerated.remove(model)
+                #logger.warning(f"Moving model {id(model):x} / {id(real_model):x} to device {get_torch_device()}")
+                real_model.to(get_torch_device())
+                #logger.warning(f"Done moving model {id(model):x} / {id(real_model):x} to device {get_torch_device()}")
+            else:
+                if vram_state == VRAMState.NO_VRAM:
+                    device_map = accelerate.infer_auto_device_map(real_model, max_memory={0: "256MiB", "cpu": "16GiB"})
+                elif vram_state == VRAMState.LOW_VRAM:
+                    device_map = accelerate.infer_auto_device_map(real_model, max_memory={0: "{}MiB".format(total_vram_available_mb), "cpu": "16GiB"})
+
+                accelerate.dispatch_model(real_model, device_map=device_map, main_device=get_torch_device())
+                self.models_accelerated.append(model)
+            return model
+
+    def load_controlnet_gpu(self, control_models):
+        with self._load_mutex:
+            global vram_state
+            if vram_state == VRAMState.CPU:
+                return
+
+            if vram_state == VRAMState.LOW_VRAM or vram_state == VRAMState.NO_VRAM:
+                #don't load controlnets like this if low vram because they will be loaded right before running and unloaded right after
+                return
+
+            models = []
+            for m in control_models:
+                models += m.get_models()
+
+            device = get_torch_device()
+            for m in models:
+                if m not in self.current_gpu_controlnets:
+                    #logger.warning(f"Loaded controlnet {id(m):x} to GPU")
+                    self.current_gpu_controlnets.append(m.to(device))
+
+    def unload_controlnet_gpu(self, control_models):
+        with self._load_mutex:
+            global vram_state
+            if vram_state == VRAMState.CPU:
+                return
+
+            if vram_state == VRAMState.LOW_VRAM or vram_state == VRAMState.NO_VRAM:
+                #don't load controlnets like this if low vram because they will be loaded right before running and unloaded right after
+                return
+
+            models = []
+            for m in control_models:
+                models += m.get_models()
+
+            for m in models:
+                if m in self.current_gpu_controlnets:
+                    m.cpu()
+                    self.current_gpu_controlnets.remove(m)
+                    del m
+
+model_manager = ModelManager()
 
 
 def load_if_low_vram(model):
@@ -272,6 +354,8 @@ def get_free_memory(dev=None, torch_free_too=False):
         return mem_free_total
 
 def maximum_batch_area():
+    return 0
+    # See https://github.com/jug-dev/hordelib/issues/225
     global vram_state
     if vram_state == VRAMState.NO_VRAM:
         return 0
diff --git a/comfy/samplers.py b/comfy/samplers.py
index 1552722..0ac5903 100644
--- a/comfy/samplers.py
+++ b/comfy/samplers.py
@@ -6,6 +6,8 @@ import contextlib
 from comfy import model_management
 from .ldm.models.diffusion.ddim import DDIMSampler
 from .ldm.modules.diffusionmodules.util import make_ddim_timesteps
+from loguru import logger
+import threading
 
 #The main sampling function shared by all the samplers
 #Returns predicted noise
@@ -126,6 +128,7 @@ def sampling_function(model_function, x, timestep, uncond, cond, cond_scale, con
             return out
 
         def calc_cond_uncond_batch(model_function, cond, uncond, x_in, timestep, max_total_area, cond_concat_in, model_options):
+            # #logger.warning("calc_cond_uncond_batch()")
             out_cond = torch.zeros_like(x_in)
             out_count = torch.ones_like(x_in)/100000.0
 
@@ -136,12 +139,14 @@ def sampling_function(model_function, x, timestep, uncond, cond, cond_scale, con
             UNCOND = 1
 
             to_run = []
+            # #logger.warning("calc_cond_uncond_batch() 1")
             for x in cond:
                 p = get_area_and_mult(x, x_in, cond_concat_in, timestep)
                 if p is None:
                     continue
 
                 to_run += [(p, COND)]
+            #logger.warning("calc_cond_uncond_batch() 2")
             for x in uncond:
                 p = get_area_and_mult(x, x_in, cond_concat_in, timestep)
                 if p is None:
@@ -149,6 +154,7 @@ def sampling_function(model_function, x, timestep, uncond, cond, cond_scale, con
 
                 to_run += [(p, UNCOND)]
 
+            #logger.warning("calc_cond_uncond_batch() 3")
             while len(to_run) > 0:
                 first = to_run[0]
                 first_shape = first[0][0].shape
@@ -157,15 +163,18 @@ def sampling_function(model_function, x, timestep, uncond, cond, cond_scale, con
                     if can_concat_cond(to_run[x][0], first[0]):
                         to_batch_temp += [x]
 
+                #logger.warning("calc_cond_uncond_batch() 3a")
                 to_batch_temp.reverse()
                 to_batch = to_batch_temp[:1]
 
+                #logger.warning("calc_cond_uncond_batch() 3b")
                 for i in range(1, len(to_batch_temp) + 1):
                     batch_amount = to_batch_temp[:len(to_batch_temp)//i]
                     if (len(batch_amount) * first_shape[0] * first_shape[2] * first_shape[3] < max_total_area):
                         to_batch = batch_amount
                         break
 
+                #logger.warning("calc_cond_uncond_batch() 3c")
                 input_x = []
                 mult = []
                 c = []
@@ -201,11 +210,16 @@ def sampling_function(model_function, x, timestep, uncond, cond, cond_scale, con
 
                 c['transformer_options'] = transformer_options
 
+                model_management.model_manager.sampler_mutex.acquire()
                 output = model_function(input_x, timestep_, cond=c).chunk(batch_chunks)
+                #logger.warning(f"{threading.current_thread().ident}: calc_cond_uncond_batch() 3d del")
                 del input_x
+                model_management.model_manager.sampler_mutex.release()
 
-                model_management.throw_exception_if_processing_interrupted()
-
+                #logger.warning(f"{threading.current_thread().ident}: calc_cond_uncond_batch() 3d(comfy)")
+                # model_management.throw_exception_if_processing_interrupted()
+                
+                #logger.warning("calc_cond_uncond_batch() 3f")
                 for o in range(batch_chunks):
                     if cond_or_uncond[o] == COND:
                         out_cond[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += output[o] * mult[o]
@@ -213,17 +227,24 @@ def sampling_function(model_function, x, timestep, uncond, cond, cond_scale, con
                     else:
                         out_uncond[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += output[o] * mult[o]
                         out_uncond_count[:,:,area[o][2]:area[o][0] + area[o][2],area[o][3]:area[o][1] + area[o][3]] += mult[o]
+                #logger.warning("calc_cond_uncond_batch() 3g")
                 del mult
+                #logger.warning("calc_cond_uncond_batch() 3h")
+
 
+            #logger.warning("calc_cond_uncond_batch() 4")
             out_cond /= out_count
             del out_count
+            #logger.warning("calc_cond_uncond_batch() 5")
             out_uncond /= out_uncond_count
             del out_uncond_count
+            #logger.warning("calc_cond_uncond_batch() 6")
 
             return out_cond, out_uncond
 
-
+        #logger.warning("Entering main shared sampling function")
         max_total_area = model_management.maximum_batch_area()
+        #logger.warning("Entering main shared sampling function 2")
         cond, uncond = calc_cond_uncond_batch(model_function, cond, uncond, x, timestep, max_total_area, cond_concat, model_options)
         if "sampler_cfg_function" in model_options:
             return model_options["sampler_cfg_function"](cond, uncond, cond_scale)
@@ -458,12 +479,14 @@ class KSampler:
         sigmas = self.sigmas
         sigma_min = self.sigma_min
 
+        #logger.warning("sampler step 1")
         if last_step is not None and last_step < (len(sigmas) - 1):
             sigma_min = sigmas[last_step]
             sigmas = sigmas[:last_step + 1]
             if force_full_denoise:
                 sigmas[-1] = 0
 
+        #logger.warning("sampler step 2")
         if start_step is not None:
             if start_step < (len(sigmas) - 1):
                 sigmas = sigmas[start_step:]
@@ -473,6 +496,7 @@ class KSampler:
                 else:
                     return torch.zeros_like(noise)
 
+        #logger.warning("sampler step 3")
         positive = positive[:]
         negative = negative[:]
         #make sure each cond area has an opposite one with the same area
@@ -484,11 +508,13 @@ class KSampler:
         apply_empty_x_to_equal_area(positive, negative, 'control', lambda cond_cnets, x: cond_cnets[x])
         apply_empty_x_to_equal_area(positive, negative, 'gligen', lambda cond_cnets, x: cond_cnets[x])
 
+        #logger.warning("sampler step 5")
         if self.model.model.diffusion_model.dtype == torch.float16:
             precision_scope = torch.autocast
         else:
             precision_scope = contextlib.nullcontext
 
+        #logger.warning("sampler step 6")
         if hasattr(self.model, 'noise_augmentor'): #unclip
             positive = encode_adm(self.model.noise_augmentor, positive, noise.shape[0], self.device)
             negative = encode_adm(self.model.noise_augmentor, negative, noise.shape[0], self.device)
@@ -516,7 +542,9 @@ class KSampler:
         else:
             max_denoise = True
 
+        #logger.warning("sampler step 7")
         with precision_scope(model_management.get_autocast_device(self.device)):
+            #logger.warning("sampler step 7a")
             if self.sampler == "uni_pc":
                 samples = uni_pc.sample_unipc(self.model_wrap, noise, latent_image, sigmas, sampling_function=sampling_function, max_denoise=max_denoise, extra_args=extra_args, noise_mask=denoise_mask)
             elif self.sampler == "uni_pc_bh2":
@@ -532,22 +560,23 @@ class KSampler:
                 sampler.make_schedule_timesteps(ddim_timesteps=timesteps, verbose=False)
                 z_enc = sampler.stochastic_encode(latent_image, torch.tensor([len(timesteps) - 1] * noise.shape[0]).to(self.device), noise=noise, max_denoise=max_denoise)
                 samples, _ = sampler.sample_custom(ddim_timesteps=timesteps,
-                                                     conditioning=positive,
-                                                     batch_size=noise.shape[0],
-                                                     shape=noise.shape[1:],
-                                                     verbose=False,
-                                                     unconditional_guidance_scale=cfg,
-                                                     unconditional_conditioning=negative,
-                                                     eta=0.0,
-                                                     x_T=z_enc,
-                                                     x0=latent_image,
-                                                     denoise_function=sampling_function,
-                                                     extra_args=extra_args,
-                                                     mask=noise_mask,
-                                                     to_zero=sigmas[-1]==0,
-                                                     end_step=sigmas.shape[0] - 1)
+                                                    conditioning=positive,
+                                                    batch_size=noise.shape[0],
+                                                    shape=noise.shape[1:],
+                                                    verbose=False,
+                                                    unconditional_guidance_scale=cfg,
+                                                    unconditional_conditioning=negative,
+                                                    eta=0.0,
+                                                    x_T=z_enc,
+                                                    x0=latent_image,
+                                                    denoise_function=sampling_function,
+                                                    extra_args=extra_args,
+                                                    mask=noise_mask,
+                                                    to_zero=sigmas[-1]==0,
+                                                    end_step=sigmas.shape[0] - 1)
 
             else:
+                #logger.warning("sampler step 7b")
                 extra_args["denoise_mask"] = denoise_mask
                 self.model_k.latent_image = latent_image
                 self.model_k.noise = noise
@@ -557,10 +586,13 @@ class KSampler:
                 if latent_image is not None:
                     noise += latent_image
                 if self.sampler == "dpm_fast":
+                    #logger.warning("sampler sample_dpm_fast")
                     samples = k_diffusion_sampling.sample_dpm_fast(self.model_k, noise, sigma_min, sigmas[0], self.steps, extra_args=extra_args)
                 elif self.sampler == "dpm_adaptive":
+                    #logger.warning("sampler sample_dpm_adaptive")
                     samples = k_diffusion_sampling.sample_dpm_adaptive(self.model_k, noise, sigma_min, sigmas[0], extra_args=extra_args)
                 else:
+                    #logger.warning("sampler: "+"sample_{}".format(self.sampler))
                     samples = getattr(k_diffusion_sampling, "sample_{}".format(self.sampler))(self.model_k, noise, sigmas, extra_args=extra_args)
 
         return samples.to(torch.float32)
diff --git a/comfy/sd.py b/comfy/sd.py
index 211acd7..b175da5 100644
--- a/comfy/sd.py
+++ b/comfy/sd.py
@@ -1,6 +1,7 @@
 import torch
 import contextlib
 import copy
+import threading
 
 import sd1_clip
 import sd2_clip
@@ -13,6 +14,8 @@ from .t2i_adapter import adapter
 
 from . import utils
 from . import clip_vision
+from loguru import logger
+import threading
 from . import gligen
 
 def load_model_weights(model, sd, verbose=False, load_state_dict_to=[]):
@@ -269,6 +272,7 @@ class ModelPatcher:
     def patch_model(self):
         model_sd = self.model.state_dict()
         for p in self.patches:
+            #logger.warning(f"Patching model {id(self):x} {len(self.patches)} patches")
             for k in p[1]:
                 v = p[1][k]
                 key = k
@@ -423,52 +427,72 @@ class VAE:
         return output
 
     def decode(self, samples_in):
-        model_management.unload_model()
-        self.first_stage_model = self.first_stage_model.to(self.device)
-        try:
-            free_memory = model_management.get_free_memory(self.device)
-            batch_number = int((free_memory * 0.7) / (2562 * samples_in.shape[2] * samples_in.shape[3] * 64))
-            batch_number = max(1, batch_number)
-
-            pixel_samples = torch.empty((samples_in.shape[0], 3, round(samples_in.shape[2] * 8), round(samples_in.shape[3] * 8)), device="cpu")
-            for x in range(0, samples_in.shape[0], batch_number):
-                samples = samples_in[x:x+batch_number].to(self.device)
-                pixel_samples[x:x+batch_number] = torch.clamp((self.first_stage_model.decode(1. / self.scale_factor * samples) + 1.0) / 2.0, min=0.0, max=1.0).cpu()
-        except model_management.OOM_EXCEPTION as e:
-            print("Warning: Ran out of memory when regular VAE decoding, retrying with tiled VAE decoding.")
-            pixel_samples = self.decode_tiled_(samples_in)
-
-        self.first_stage_model = self.first_stage_model.cpu()
-        pixel_samples = pixel_samples.cpu().movedim(1,-1)
-        return pixel_samples
+        with model_management.model_manager.vae_mutex:
+            tid = threading.current_thread().ident
+            #logger.warning(f"VAE.decode({tid}) would unload model")
+            # model_management.unload_model()
+
+            #logger.warning(f"VAE.decode({tid}) first_stage_model to device {self.device}")
+            self.first_stage_model = self.first_stage_model.to(self.device)
+            try:
+                #logger.warning(f"VAE.decode({tid}) get free memory")
+                free_memory = model_management.get_free_memory(self.device)
+                batch_number = int((free_memory * 0.7) / (2562 * samples_in.shape[2] * samples_in.shape[3] * 64))
+                batch_number = max(1, batch_number)
+                #logger.warning(f"VAE.decode({tid}) {batch_number} batches")
+
+                #logger.warning(f"VAE.decode({tid}) torch.empty")
+                pixel_samples = torch.empty((samples_in.shape[0], 3, round(samples_in.shape[2] * 8), round(samples_in.shape[3] * 8)), device="cpu")
+                #logger.warning(f"VAE.decode({tid}) starting iterations")
+                for x in range(0, samples_in.shape[0], batch_number):
+                    samples = samples_in[x:x+batch_number].to(self.device)
+                    #logger.warning(f"VAE.decode({tid}) got samples")
+                    pixel_samples[x:x+batch_number] = torch.clamp((self.first_stage_model.decode(1. / self.scale_factor * samples) + 1.0) / 2.0, min=0.0, max=1.0).cpu()
+                    #logger.warning(f"VAE.decode({tid}) clamped")
+            except model_management.OOM_EXCEPTION as e:
+                print("Warning: Ran out of memory when regular VAE decoding, retrying with tiled VAE decoding.")
+                pixel_samples = self.decode_tiled_(samples_in)
+
+            #logger.warning(f"VAE.decode({tid}) moving to cpu")
+            self.first_stage_model = self.first_stage_model.cpu()
+            #logger.warning(f"VAE.decode({tid}) calculating pixels")
+            pixel_samples = pixel_samples.cpu().movedim(1,-1)
+            #logger.warning(f"VAE.decode({tid}) returning")
+            return pixel_samples
 
     def decode_tiled(self, samples, tile_x=64, tile_y=64, overlap = 16):
-        model_management.unload_model()
-        self.first_stage_model = self.first_stage_model.to(self.device)
-        output = self.decode_tiled_(samples, tile_x, tile_y, overlap)
-        self.first_stage_model = self.first_stage_model.cpu()
-        return output.movedim(1,-1)
+        with model_management.model_manager.vae_mutex:
+            #logger.warning("VAE.decode_tiled() would unload model")
+            # model_management.unload_model()
+            self.first_stage_model = self.first_stage_model.to(self.device)
+            output = self.decode_tiled_(samples, tile_x, tile_y, overlap)
+            self.first_stage_model = self.first_stage_model.cpu()
+            return output.movedim(1,-1)
 
     def encode(self, pixel_samples):
-        model_management.unload_model()
-        self.first_stage_model = self.first_stage_model.to(self.device)
-        pixel_samples = pixel_samples.movedim(-1,1).to(self.device)
-        samples = self.first_stage_model.encode(2. * pixel_samples - 1.).sample() * self.scale_factor
-        self.first_stage_model = self.first_stage_model.cpu()
-        samples = samples.cpu()
-        return samples
+        #logger.warning("VAE.encode() would unload model")
+        # model_management.unload_model()
+        with model_management.model_manager.vae_mutex:
+            self.first_stage_model = self.first_stage_model.to(self.device)
+            pixel_samples = pixel_samples.movedim(-1,1).to(self.device)
+            samples = self.first_stage_model.encode(2. * pixel_samples - 1.).sample() * self.scale_factor
+            self.first_stage_model = self.first_stage_model.cpu()
+            samples = samples.cpu()
+            return samples
 
     def encode_tiled(self, pixel_samples, tile_x=512, tile_y=512, overlap = 64):
-        model_management.unload_model()
-        self.first_stage_model = self.first_stage_model.to(self.device)
-        pixel_samples = pixel_samples.movedim(-1,1).to(self.device)
-        samples = utils.tiled_scale(pixel_samples, lambda a: self.first_stage_model.encode(2. * a - 1.).sample() * self.scale_factor, tile_x, tile_y, overlap, upscale_amount = (1/8), out_channels=4)
-        samples += utils.tiled_scale(pixel_samples, lambda a: self.first_stage_model.encode(2. * a - 1.).sample() * self.scale_factor, tile_x * 2, tile_y // 2, overlap, upscale_amount = (1/8), out_channels=4)
-        samples += utils.tiled_scale(pixel_samples, lambda a: self.first_stage_model.encode(2. * a - 1.).sample() * self.scale_factor, tile_x // 2, tile_y * 2, overlap, upscale_amount = (1/8), out_channels=4)
-        samples /= 3.0
-        self.first_stage_model = self.first_stage_model.cpu()
-        samples = samples.cpu()
-        return samples
+        #logger.warning("VAE.decode() would unload model")
+        # model_management.unload_model()
+        with model_management.model_manager.vae_mutex:
+            self.first_stage_model = self.first_stage_model.to(self.device)
+            pixel_samples = pixel_samples.movedim(-1,1).to(self.device)
+            samples = utils.tiled_scale(pixel_samples, lambda a: self.first_stage_model.encode(2. * a - 1.).sample() * self.scale_factor, tile_x, tile_y, overlap, upscale_amount = (1/8), out_channels=4)
+            samples += utils.tiled_scale(pixel_samples, lambda a: self.first_stage_model.encode(2. * a - 1.).sample() * self.scale_factor, tile_x * 2, tile_y // 2, overlap, upscale_amount = (1/8), out_channels=4)
+            samples += utils.tiled_scale(pixel_samples, lambda a: self.first_stage_model.encode(2. * a - 1.).sample() * self.scale_factor, tile_x // 2, tile_y * 2, overlap, upscale_amount = (1/8), out_channels=4)
+            samples /= 3.0
+            self.first_stage_model = self.first_stage_model.cpu()
+            samples = samples.cpu()
+            return samples
 
 def resize_image_to(tensor, target_latent_tensor, batched_number):
     tensor = utils.common_upscale(tensor, target_latent_tensor.shape[3] * 8, target_latent_tensor.shape[2] * 8, 'nearest-exact', "center")
@@ -576,8 +600,15 @@ class ControlNet:
         out.append(self.control_model)
         return out
 
+_controlnet_models = {}
+
 def load_controlnet(ckpt_path, model=None):
-    controlnet_data = utils.load_torch_file(ckpt_path)
+    if ckpt_path in _controlnet_models:
+        controlnet_data = copy.deepcopy(_controlnet_models.get(ckpt_path))
+    else:
+        controlnet_data = utils.load_torch_file(ckpt_path)
+        _controlnet_models[ckpt_path] = copy.deepcopy(controlnet_data)
+
     pth_key = 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight'
     pth = False
     sd2 = False
@@ -637,6 +668,7 @@ def load_controlnet(ckpt_path, model=None):
             if model is not None:
                 m = model.patch_model()
                 model_sd = m.state_dict()
+                #logger.warning(f"Modifying model {id(model):x} with controlnet {id(controlnet_data):x}")
                 for x in controlnet_data:
                     c_m = "control_model."
                     if x.startswith(c_m):
diff --git a/comfy_extras/nodes_upscale_model.py b/comfy_extras/nodes_upscale_model.py
index d875469..08e54ad 100644
--- a/comfy_extras/nodes_upscale_model.py
+++ b/comfy_extras/nodes_upscale_model.py
@@ -4,6 +4,7 @@ from comfy import model_management
 import torch
 import comfy.utils
 import folder_paths
+import threading
 
 class UpscaleModelLoader:
     @classmethod
@@ -23,6 +24,7 @@ class UpscaleModelLoader:
 
 
 class ImageUpscaleWithModel:
+    _mutex = threading.Lock()
     @classmethod
     def INPUT_TYPES(s):
         return {"required": { "upscale_model": ("UPSCALE_MODEL",),
@@ -34,13 +36,14 @@ class ImageUpscaleWithModel:
     CATEGORY = "image/upscaling"
 
     def upscale(self, upscale_model, image):
-        device = model_management.get_torch_device()
-        upscale_model.to(device)
-        in_img = image.movedim(-1,-3).to(device)
-        s = comfy.utils.tiled_scale(in_img, lambda a: upscale_model(a), tile_x=128 + 64, tile_y=128 + 64, overlap = 8, upscale_amount=upscale_model.scale)
-        upscale_model.cpu()
-        s = torch.clamp(s.movedim(-3,-1), min=0, max=1.0)
-        return (s,)
+        with ImageUpscaleWithModel._mutex:
+            device = model_management.get_torch_device()
+            upscale_model.to(device)
+            in_img = image.movedim(-1,-3).to(device)
+            s = comfy.utils.tiled_scale(in_img, lambda a: upscale_model(a), tile_x=128 + 64, tile_y=128 + 64, overlap = 8, upscale_amount=upscale_model.scale)
+            upscale_model.cpu()
+            s = torch.clamp(s.movedim(-3,-1), min=0, max=1.0)
+            return (s,)
 
 NODE_CLASS_MAPPINGS = {
     "UpscaleModelLoader": UpscaleModelLoader,
diff --git a/execution.py b/execution.py
index 73be6db..2c31fd9 100644
--- a/execution.py
+++ b/execution.py
@@ -7,6 +7,7 @@ import heapq
 import traceback
 import gc
 
+from loguru import logger
 import torch
 import nodes
 
@@ -183,7 +184,7 @@ class PromptExecutor:
                             if valid:
                                 executed += recursive_execute(self.server, prompt, self.outputs, x, extra_data)
             except Exception as e:
-                print(traceback.format_exc())
+                logger.error(traceback.format_exc())
                 to_delete = []
                 for o in self.outputs:
                     if o not in current_outputs:
@@ -203,8 +204,8 @@ class PromptExecutor:
                 if self.server.client_id is not None:
                     self.server.send_sync("executing", { "node": None }, self.server.client_id)
 
-        gc.collect()
-        comfy.model_management.soft_empty_cache()
+        # gc.collect()
+        # comfy.model_management.soft_empty_cache()
 
 
 def validate_inputs(prompt, item):
@@ -280,7 +281,7 @@ def validate_prompt(prompt):
         if valid == True:
             good_outputs.add(x)
         else:
-            print("Failed to validate prompt for output {} {}".format(o, reason))
+            logger.error("Failed to validate prompt for output {} {}".format(o, reason))
             print("output will be ignored")
             errors += [(o, reason)]
 
@@ -288,6 +289,8 @@ def validate_prompt(prompt):
         errors_list = "\n".join(set(map(lambda a: "{}".format(a[1]), errors)))
         return (False, "Prompt has no properly connected outputs\n {}".format(errors_list))
 
+    with open("../comfy-prompt.json", "wt", encoding="utf-8") as f:
+        f.write(json.dumps(prompt, indent=4))
     return (True, "")
 
 
diff --git a/main.py b/main.py
index 02c700e..507d1db 100644
--- a/main.py
+++ b/main.py
@@ -25,7 +25,7 @@ import yaml
 import execution
 import folder_paths
 import server
-from nodes import init_custom_nodes
+from nodes import init_custom_nodes, load_custom_nodes
 
 
 def prompt_worker(q, server):
diff --git a/nodes.py b/nodes.py
index 48c3ee9..5fe17c1 100644
--- a/nodes.py
+++ b/nodes.py
@@ -26,6 +26,11 @@ import importlib
 
 import folder_paths
 
+import pickle
+import time
+from loguru import logger
+import threading
+
 def before_node_execution():
     comfy.model_management.throw_exception_if_processing_interrupted()
 
@@ -739,7 +744,6 @@ class SetLatentNoiseMask:
         s["noise_mask"] = mask
         return (s,)
 
-
 def common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):
     latent_image = latent["samples"]
     noise_mask = None
@@ -765,55 +769,74 @@ def common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive,
         noise_mask = torch.cat([noise_mask] * noise.shape[0])
         noise_mask = noise_mask.to(device)
 
-    real_model = None
-    comfy.model_management.load_model_gpu(model)
-    real_model = model.model
-
-    noise = noise.to(device)
-    latent_image = latent_image.to(device)
-
-    positive_copy = []
-    negative_copy = []
-
-    control_nets = []
-    def get_models(cond):
-        models = []
-        for c in cond:
-            if 'control' in c[1]:
-                models += [c[1]['control']]
-            if 'gligen' in c[1]:
-                models += [c[1]['gligen'][1]]
-        return models
-
-    for p in positive:
-        t = p[0]
-        if t.shape[0] < noise.shape[0]:
-            t = torch.cat([t] * noise.shape[0])
-        t = t.to(device)
-        positive_copy += [[t] + p[1:]]
-    for n in negative:
-        t = n[0]
-        if t.shape[0] < noise.shape[0]:
-            t = torch.cat([t] * noise.shape[0])
-        t = t.to(device)
-        negative_copy += [[t] + n[1:]]
-
-    models = get_models(positive) + get_models(negative)
-    comfy.model_management.load_controlnet_gpu(models)
-
-    if sampler_name in comfy.samplers.KSampler.SAMPLERS:
-        sampler = comfy.samplers.KSampler(real_model, steps=steps, device=device, sampler=sampler_name, scheduler=scheduler, denoise=denoise, model_options=model.model_options)
-    else:
-        #other samplers
-        pass
-
-    samples = sampler.sample(noise, positive_copy, negative_copy, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask)
-    samples = samples.cpu()
-    for m in models:
-        m.cleanup()
-
-    out = latent.copy()
-    out["samples"] = samples
+    comfy.model_management.model_manager.set_model_in_use(model)
+    try:
+        comfy.model_management.model_manager.load_model_gpu(model)
+        real_model = model.model
+
+        #logger.warning(f"Moving noise {id(noise):x} to device {device}")
+        noise = noise.to(device)
+        #logger.warning(f"Moving latest image {id(latent_image):x} to device {device}")
+        latent_image = latent_image.to(device)
+
+        positive_copy = []
+        negative_copy = []
+
+        #logger.warning(f"Assembling control nets")
+        control_nets = []
+        def get_models(cond):
+            models = []
+            for c in cond:
+                if 'control' in c[1]:
+                    models += [c[1]['control']]
+                if 'gligen' in c[1]:
+                    models += [c[1]['gligen'][1]]
+            return models
+
+        for p in positive:
+            t = p[0]
+            if t.shape[0] < noise.shape[0]:
+                t = torch.cat([t] * noise.shape[0])
+            t = t.to(device)
+            positive_copy += [[t] + p[1:]]
+        for n in negative:
+            t = n[0]
+            if t.shape[0] < noise.shape[0]:
+                t = torch.cat([t] * noise.shape[0])
+            t = t.to(device)
+            negative_copy += [[t] + n[1:]]
+
+        models = get_models(positive) + get_models(negative)
+        if models:
+            comfy.model_management.model_manager.load_controlnet_gpu(models)
+            comfy.model_management.model_manager.sampler_mutex.acquire()
+
+        if sampler_name in comfy.samplers.KSampler.SAMPLERS:
+            #logger.warning(f"Creating KSampler for real model {id(real_model):x}")
+            sampler = comfy.samplers.KSampler(real_model, steps=steps, device=device, sampler=sampler_name, scheduler=scheduler, denoise=denoise, model_options=model.model_options)
+        else:
+            #other samplers
+            pass
+
+        #logger.warning(f"Sampling")
+        samples = sampler.sample(noise, positive_copy, negative_copy, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask)
+        #logger.warning("Done sampling")
+        samples = samples.cpu()
+
+        #logger.warning("Moved samples to cpu")
+        if models:
+            # XXX force unload models for controlnet
+            #logger.warning("Forcing GPU unload of controlnet models")
+            comfy.model_management.model_manager.unload_controlnet_gpu(models)
+            comfy.model_management.model_manager.unload_model(model)
+            comfy.model_management.model_manager.sampler_mutex.release()
+            for m in models:
+                m.cleanup()
+
+        out = latent.copy()
+        out["samples"] = samples
+    finally:
+        comfy.model_management.model_manager.done_with_model(model)
     return (out, )
 
 class KSampler:
@@ -976,6 +999,7 @@ class LoadImage:
     def load_image(self, image):
         input_dir = folder_paths.get_input_directory()
         image_path = os.path.join(input_dir, image)
+        #logger.warning(f"Loading image {image_path}")
         i = Image.open(image_path)
         image = i.convert("RGB")
         image = np.array(image).astype(np.float32) / 255.0
